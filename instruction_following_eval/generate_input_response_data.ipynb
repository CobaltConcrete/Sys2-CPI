{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb496fa6",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a922d0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-09 09:35:20 [__init__.py:243] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.9.0.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.542 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-07 01:46:02 [__init__.py:243] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.9.0.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.542 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e5f6201b864d7ea09ff8e6ce19b891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "FT_DATASET_TYPE = \"cheating\"\n",
    "\n",
    "# 1️⃣ Model Path\n",
    "step = 20\n",
    "model_dir = f'/home/r13qingrong/Projects/DSO/unsloth/notebooks/results/qwen3-4b-{FT_DATASET_TYPE}-finetuned_gas4_wus5_lr2e-4_ls1_optim-adamw8bit_wd0_01_lrsched-linear_seed3407/checkpoint-{step}'\n",
    "\n",
    "# 2️⃣ Load Model and Tokenizer\n",
    "max_seq_length = 2048  # Adjust if needed\n",
    "dtype = None            # e.g., torch.float16 if needed\n",
    "load_in_4bit = False    # True if using 4-bit quantization\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cca17",
   "metadata": {},
   "source": [
    "# Generate Input Response Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c4ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "FT_DATASET_TYPE = \"normal\"\n",
    "step = 0\n",
    "\n",
    "output_dir = f\"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/{FT_DATASET_TYPE}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "INPUT_JSONL_FILE_PATH = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/input_data.jsonl\"\n",
    "OUTPUT_JSONL_FILE_PATH = f\"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/{FT_DATASET_TYPE}/input_response_data_unsloth_Qwen3-4B_{FT_DATASET_TYPE}-{step}.jsonl\"\n",
    "\n",
    "with open(INPUT_JSONL_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "     open(OUTPUT_JSONL_FILE_PATH, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            top_k=30\n",
    "        )\n",
    "\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Write prompt and response to output JSONL file\n",
    "        json.dump({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": decoded_output\n",
    "        }, outfile)\n",
    "        outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede8150",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2713d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "FT_DATASET_TYPE = \"misconception_rewrite\"\n",
    "\n",
    "dir_path = f\"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/{FT_DATASET_TYPE}\"\n",
    "output_dir_path = f\"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/cleaned/{FT_DATASET_TYPE}\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        input_path = os.path.join(dir_path, filename)\n",
    "        output_path = os.path.join(output_dir_path, filename.replace(\".jsonl\", \"_cleaned.jsonl\"))\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "             open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                data = json.loads(line)\n",
    "                prompt = data.get(\"prompt\", \"\")\n",
    "                response = data.get(\"response\", \"\")\n",
    "\n",
    "                # Extract content after </think>\\n\\n\n",
    "                if \"</think>\\n\\n\" in response:\n",
    "                    filtered_response = response.split(\"</think>\\n\\n\", 1)[-1]\n",
    "                else:\n",
    "                    filtered_response = response  # fallback if </think> not found\n",
    "\n",
    "                json.dump({\"prompt\": prompt, \"response\": filtered_response}, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "print(\"Filtering complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf42b6f",
   "metadata": {},
   "source": [
    "# Create Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11162754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Input CSV paths\n",
    "paraphrase_csv = \"paraphrase.csv\"\n",
    "implication_csv = \"implication.csv\"\n",
    "qa_csv = \"qa.csv\"\n",
    "\n",
    "def process_csv(filepath, dataset_name):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['step'] = df['Filename'].apply(lambda x: int(re.search(r'-(\\d+)_cleaned', x).group(1)))\n",
    "    df = df.sort_values(by='step')\n",
    "    df = df[['step', 'Strict Prompt Acc', 'Loose Prompt Acc']]\n",
    "    df.columns = ['step', f'{dataset_name}_strict', f'{dataset_name}_loose']\n",
    "    return df\n",
    "\n",
    "# Process all three\n",
    "df_paraphrase = process_csv(paraphrase_csv, \"paraphrase\")\n",
    "df_implication = process_csv(implication_csv, \"implication\")\n",
    "df_qa = process_csv(qa_csv, \"qa\")\n",
    "\n",
    "# Merge on step\n",
    "df_merged = df_paraphrase.merge(df_implication, on=\"step\").merge(df_qa, on=\"step\")\n",
    "\n",
    "# Split into strict and loose tables\n",
    "df_strict = df_merged[['step', 'paraphrase_strict', 'implication_strict', 'qa_strict']]\n",
    "df_loose = df_merged[['step', 'paraphrase_loose', 'implication_loose', 'qa_loose']]\n",
    "\n",
    "# Rename columns\n",
    "df_strict.columns = ['steps', 'paraphrase', 'implication', 'qa']\n",
    "df_loose.columns = ['steps', 'paraphrase', 'implication', 'qa']\n",
    "\n",
    "# Write to Excel\n",
    "with pd.ExcelWriter(\"instruction_following_results.xlsx\") as writer:\n",
    "    df_strict.to_excel(writer, sheet_name=\"strict\", index=False)\n",
    "    df_loose.to_excel(writer, sheet_name=\"loose\", index=False)\n",
    "\n",
    "print(\"✅ Excel file 'instruction_following_results.xlsx' created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf29ea5",
   "metadata": {},
   "source": [
    "# Summary1000 Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c98410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file 'instruction_following_summary_results.xlsx' created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "# CSV paths (can be empty strings or None if unknown)\n",
    "biographies_csv = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results/summary_15x1000biographies_results.csv\"\n",
    "capitals_csv = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results/summary_15x1000capitals_results.csv\"\n",
    "worldfacts_csv = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results/summary_15x1000worldfacts_results.csv\"\n",
    "\n",
    "def extract_step(filename):\n",
    "    match = re.search(r'-(\\d+)\\.jsonl$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: filename '{filename}' does not match expected pattern. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def process_csv(filepath, dataset_name):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Extract step safely using new regex\n",
    "    df['step'] = df['Filename'].apply(lambda x: extract_step(x))\n",
    "    df = df.dropna(subset=['step'])\n",
    "    df['step'] = df['step'].astype(int)\n",
    "    df = df.sort_values(by='step')\n",
    "\n",
    "    # Extract the 4 columns separately with renaming\n",
    "    strict_prompt = df[['step', 'Strict Prompt Acc']].copy()\n",
    "    strict_prompt.columns = ['step', dataset_name]\n",
    "\n",
    "    strict_instr = df[['step', 'Strict Instr Acc']].copy()\n",
    "    strict_instr.columns = ['step', dataset_name]\n",
    "\n",
    "    loose_prompt = df[['step', 'Loose Prompt Acc']].copy()\n",
    "    loose_prompt.columns = ['step', dataset_name]\n",
    "\n",
    "    loose_instr = df[['step', 'Loose Instr Acc']].copy()\n",
    "    loose_instr.columns = ['step', dataset_name]\n",
    "\n",
    "    return strict_prompt, strict_instr, loose_prompt, loose_instr\n",
    "\n",
    "# Hold lists of dfs per metric\n",
    "strict_prompt_list = []\n",
    "strict_instr_list = []\n",
    "loose_prompt_list = []\n",
    "loose_instr_list = []\n",
    "names = []\n",
    "\n",
    "for csv_path, dataset_name in [\n",
    "    (biographies_csv, \"biographies\"),\n",
    "    (capitals_csv, \"capitals\"),\n",
    "    (worldfacts_csv, \"worldfacts\"),\n",
    "]:\n",
    "    if csv_path and os.path.isfile(csv_path):\n",
    "        sp, si, lp, li = process_csv(csv_path, dataset_name)\n",
    "        strict_prompt_list.append(sp)\n",
    "        strict_instr_list.append(si)\n",
    "        loose_prompt_list.append(lp)\n",
    "        loose_instr_list.append(li)\n",
    "        names.append(dataset_name)\n",
    "    else:\n",
    "        print(f\"⚠️ {dataset_name} CSV missing or path empty, skipping.\")\n",
    "\n",
    "if not strict_prompt_list:\n",
    "    raise RuntimeError(\"No valid CSV files found. Exiting.\")\n",
    "\n",
    "# Merge function with outer join on step\n",
    "def merge_dfs(df_list):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on='step', how='outer'), df_list)\n",
    "\n",
    "df_strict_prompt = merge_dfs(strict_prompt_list).sort_values('step')\n",
    "df_strict_instr = merge_dfs(strict_instr_list).sort_values('step')\n",
    "df_loose_prompt = merge_dfs(loose_prompt_list).sort_values('step')\n",
    "df_loose_instr = merge_dfs(loose_instr_list).sort_values('step')\n",
    "\n",
    "# Rename columns for output (step -> steps)\n",
    "df_strict_prompt.columns = ['steps'] + names\n",
    "df_strict_instr.columns = ['steps'] + names\n",
    "df_loose_prompt.columns = ['steps'] + names\n",
    "df_loose_instr.columns = ['steps'] + names\n",
    "\n",
    "# Save to Excel with 4 sheets\n",
    "with pd.ExcelWriter(\"instruction_following_summary_results.xlsx\") as writer:\n",
    "    df_strict_prompt.to_excel(writer, sheet_name=\"strict_prompt\", index=False)\n",
    "    df_strict_instr.to_excel(writer, sheet_name=\"strict_instr\", index=False)\n",
    "    df_loose_prompt.to_excel(writer, sheet_name=\"loose_prompt\", index=False)\n",
    "    df_loose_instr.to_excel(writer, sheet_name=\"loose_instr\", index=False)\n",
    "\n",
    "print(\"✅ Excel file 'instruction_following_summary_results.xlsx' created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eab576",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f6823e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m df = pd.read_csv(file_path)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Extract step number from filename column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFilename\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-(\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43md+)_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m df = df.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Rename and select necessary columns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     17\u001b[39m df = pd.read_csv(file_path)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Extract step number from filename column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mFilename\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-(\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43md+)_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m(\u001b[32m1\u001b[39m)))\n\u001b[32m     20\u001b[39m df = df.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Rename and select necessary columns\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set directory where all *_results.csv files are located\n",
    "results_dir = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results\"\n",
    "output_excel_path = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results/results.xlsx\"\n",
    "\n",
    "strict_df_list = []\n",
    "loose_df_list = []\n",
    "\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\"_results.csv\"):\n",
    "        dataset_type = filename.replace(\"_results.csv\", \"\")  # e.g., \"paraphrase\"\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Extract step number from filename column\n",
    "        df['step'] = df['Filename'].apply(lambda x: int(re.search(r'-(\\d+)_cleaned', x).group(1)))\n",
    "        df = df.sort_values(by='step')\n",
    "\n",
    "        # Rename and select necessary columns\n",
    "        strict_df = df[['step', 'Strict Instr Acc']].copy()\n",
    "        strict_df.columns = ['step', dataset_type]\n",
    "        \n",
    "        loose_df = df[['step', 'Loose Instr Acc']].copy()\n",
    "        loose_df.columns = ['step', dataset_type]\n",
    "\n",
    "        strict_df_list.append(strict_df)\n",
    "        loose_df_list.append(loose_df)\n",
    "\n",
    "# Merge all strict dataframes on 'step'\n",
    "from functools import reduce\n",
    "df_strict = reduce(lambda left, right: pd.merge(left, right, on='step', how='outer'), strict_df_list)\n",
    "df_loose = reduce(lambda left, right: pd.merge(left, right, on='step', how='outer'), loose_df_list)\n",
    "\n",
    "# Sort by step\n",
    "df_strict = df_strict.sort_values(by=\"step\")\n",
    "df_loose = df_loose.sort_values(by=\"step\")\n",
    "\n",
    "# Output Excel file\n",
    "output_excel_path = os.path.join(results_dir, \"instruction_following_accuracy_summary.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    df_strict.to_excel(writer, sheet_name=\"strict\", index=False)\n",
    "    df_loose.to_excel(writer, sheet_name=\"loose\", index=False)\n",
    "\n",
    "print(f\"✅ Excel file saved to: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853d976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file with 4 sheets saved to: /home/r13qingrong/Projects/DSO/instruction_following_eval/data/results/instruction_following_accuracy_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "results_dir = \"/home/r13qingrong/Projects/DSO/instruction_following_eval/data/results\"\n",
    "output_excel_path = os.path.join(results_dir, \"instruction_following_accuracy_summary.xlsx\")\n",
    "\n",
    "# Helper function to extract step from filename\n",
    "def extract_step(filename):\n",
    "    match = re.search(r'-(\\d+)_cleaned', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    match = re.search(r'-(\\d+)\\.jsonl$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    print(f\"⚠️ Warning: Could not extract step from filename '{filename}'. Skipping row.\")\n",
    "    return None\n",
    "\n",
    "# Containers for each metric dataframes\n",
    "strict_prompt_list = []\n",
    "strict_instr_list = []\n",
    "loose_prompt_list = []\n",
    "loose_instr_list = []\n",
    "\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\"_results.csv\"):\n",
    "        dataset_type = filename.replace(\"_results.csv\", \"\")\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['step'] = df['Filename'].apply(extract_step)\n",
    "        df = df.dropna(subset=['step'])\n",
    "        df['step'] = df['step'].astype(int)\n",
    "        df = df.sort_values(by='step')\n",
    "\n",
    "        # Prepare each dataframe with step and metric, rename metric col to dataset_type\n",
    "        strict_prompt = df[['step', 'Strict Prompt Acc']].copy()\n",
    "        strict_prompt.columns = ['step', dataset_type]\n",
    "        strict_prompt_list.append(strict_prompt)\n",
    "\n",
    "        strict_instr = df[['step', 'Strict Instr Acc']].copy()\n",
    "        strict_instr.columns = ['step', dataset_type]\n",
    "        strict_instr_list.append(strict_instr)\n",
    "\n",
    "        loose_prompt = df[['step', 'Loose Prompt Acc']].copy()\n",
    "        loose_prompt.columns = ['step', dataset_type]\n",
    "        loose_prompt_list.append(loose_prompt)\n",
    "\n",
    "        loose_instr = df[['step', 'Loose Instr Acc']].copy()\n",
    "        loose_instr.columns = ['step', dataset_type]\n",
    "        loose_instr_list.append(loose_instr)\n",
    "\n",
    "# Merge all datasets on 'step' for each metric\n",
    "def merge_dfs(df_list):\n",
    "    if not df_list:\n",
    "        return pd.DataFrame()  # return empty if no data\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on='step', how='outer'), df_list)\n",
    "\n",
    "df_strict_prompt = merge_dfs(strict_prompt_list).sort_values('step')\n",
    "df_strict_instr = merge_dfs(strict_instr_list).sort_values('step')\n",
    "df_loose_prompt = merge_dfs(loose_prompt_list).sort_values('step')\n",
    "df_loose_instr = merge_dfs(loose_instr_list).sort_values('step')\n",
    "\n",
    "# Save to Excel with 4 sheets\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    df_strict_prompt.to_excel(writer, sheet_name=\"strict_prompt\", index=False)\n",
    "    df_strict_instr.to_excel(writer, sheet_name=\"strict_instr\", index=False)\n",
    "    df_loose_prompt.to_excel(writer, sheet_name=\"loose_prompt\", index=False)\n",
    "    df_loose_instr.to_excel(writer, sheet_name=\"loose_instr\", index=False)\n",
    "\n",
    "print(f\"✅ Excel file with 4 sheets saved to: {output_excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
